# 中文训练数据集

## **分类数据集**

今日头条分类数据集

数据规模：共38万条，分布于15个分类中。

{% embed url="https://github.com/napoler/toutiao-text-classfication-dataset" %}

**清华新闻分类语料**

数据量：74万篇新闻文档（2.19 GB）

{% embed url="http://thuctc.thunlp.org/\#%E8%8E%B7%E5%8F%96%E9%93%BE%E6%8E%A5" %}

**中科大新闻分类语料库**

情感/观点/评论 倾向性分析

## **实体识别&词性标注**

**微博实体识别**

{% embed url="https://github.com/hltcoe/golden-horse" %}

**boson数据**

包含6种实体类型。

{% embed url="https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/boson" %}

**人民日报数据集**

\*\*\*\*[**https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/renMinRiBao**](https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/renMinRiBao)\*\*\*\*

链接: [https://pan.baidu.com/s/1aTWJqLyUqDpkaNreGy3nsQ](https://pan.baidu.com/s/1aTWJqLyUqDpkaNreGy3nsQ) 提取码: x33b 复制这段内容后打开百度网盘手机App，操作更方便哦

**MSRA微软亚洲研究院数据集**

5 万多条中文命名实体识别标注数据（包括地点、机构、人物）

\*\*\*\*[**https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/MSRA**](https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/MSRA)\*\*\*\*

**SIGHAN Bakeoff 2005：**一共有四个数据集，包含繁体中文和简体中文，下面是简体中文分词数据。

{% embed url="http://sighan.cs.uchicago.edu/bakeoff2005/" %}

{% embed url="http://sighan.cs.uchicago.edu/bakeoff2005/" %}

\*\*\*\*

\*\*\*\*

\*\*\*\*

**推荐系统**![](https://pic1.zhimg.com/80/v2-5e049bcd16aa9a33f90e548519f777dc_720w.jpg)

**百科数据**

**维基百科**

维基百科会定时将语料库打包发布：

数据处理博客

[https://dumps.wikimedia.org/zhwiki/](https://link.zhihu.com/?target=https%3A//dumps.wikimedia.org/zhwiki/)

**百度百科**

只能自己爬，爬取得链接：[https://pan.baidu.com/share/init?surl=i3wvfil](https://link.zhihu.com/?target=https%3A//pan.baidu.com/share/init%3Fsurl%3Di3wvfil)提取码 neqs 。

**指代消歧**

CoNLL 2012 ：[http://conll.cemantix.org/2012/data.html](https://link.zhihu.com/?target=http%3A//conll.cemantix.org/2012/data.html)

**预训练：（词向量or模型）**

**BERT**

开源代码：[https://github.com/google-research/bert](https://link.zhihu.com/?target=https%3A//github.com/google-research/bert)

模型下载：BERT-Base, Chinese: Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters

**ELMO**

开源代码：[https://github.com/allenai/bilm-tf](https://link.zhihu.com/?target=https%3A//github.com/allenai/bilm-tf)

预训练的模型：[https://allennlp.org/elmo](https://link.zhihu.com/?target=https%3A//allennlp.org/elmo)

**腾讯词向量**

腾讯AI实验室公开的中文词向量数据集包含800多万中文词汇，其中每个词对应一个200维的向量。

下载地址：[https://ai.tencent.com/ailab/nlp/embedding.html](https://link.zhihu.com/?target=https%3A//ai.tencent.com/ailab/nlp/embedding.html)

**上百种预训练中文词向量**

[https://github.com/Embedding/Chinese-Word-Vectors](https://link.zhihu.com/?target=https%3A//github.com/Embedding/Chinese-Word-Vectors)

**中文完形填空数据集**

[https://github.com/ymcui/Chinese-RC-Dataset](https://link.zhihu.com/?target=https%3A//github.com/ymcui/Chinese-RC-Dataset)

**中华古诗词数据库**

最全中华古诗词数据集，唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。

[https://github.com/chinese-poetry/chinese-poetry](https://link.zhihu.com/?target=https%3A//github.com/chinese-poetry/chinese-poetry)

**保险行业语料库**

[https://github.com/Samurais/insuranceqa-corpus-zh](https://link.zhihu.com/?target=https%3A//github.com/Samurais/insuranceqa-corpus-zh)

**汉语拆字字典**

英文可以做char embedding，中文不妨可以试试拆字

[https://github.com/kfcd/chaizi](https://link.zhihu.com/?target=https%3A//github.com/kfcd/chaizi)

**中文数据集平台**

**搜狗实验室**

搜狗实验室提供了一些高质量的中文文本数据集，时间比较早，多为2012年以前的数据。

[https://www.sogou.com/labs/resource/list\_pingce.php](https://link.zhihu.com/?target=https%3A//www.sogou.com/labs/resource/list_pingce.php)

**中科大自然语言处理与信息检索共享平台**

[http://www.nlpir.org/?action-category-catid-28](https://link.zhihu.com/?target=http%3A//www.nlpir.org/%3Faction-category-catid-28)

**中文语料小数据**

包含了中文命名实体识别、中文关系识别、中文阅读理解等一些小量数据。

[https://github.com/crownpku/Small-Chinese-Corpus](https://link.zhihu.com/?target=https%3A//github.com/crownpku/Small-Chinese-Corpus)

**维基百科数据集**

[https://dumps.wikimedia.org/](https://link.zhihu.com/?target=https%3A//dumps.wikimedia.org/)

\*\*\*\*

